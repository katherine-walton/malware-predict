from sklearn.preprocessing import OneHotEncoder
<<<<<<< HEAD
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.model_selection import train_test_split
=======
>>>>>>> e45101a122edb39597650881ad28bcf374dbbb52
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def remove_missing_features(todelete, data):
	"""
	Removes features with large amounts of missing data.
	"""
	missing = []
	for x in range(0, len(data.columns)):
		totals = (sum (data.iloc[:,x].isnull()) ) / len(data)
		missing.append(totals)
		if totals > .99:
<<<<<<< HEAD
			todelete.append(data.columns[x])
	result = pd.DataFrame(missing, index = list(data.columns), columns=['Proportion of Missing Data'])
	result = result.sort_values(by=['Proportion of Missing Data'],ascending=False)
	# print(result)
	# result.to_csv("MissingData.csv")
=======
			todelete.append(train.columns[x])
>>>>>>> e45101a122edb39597650881ad28bcf374dbbb52
	return todelete

def remove_skewed_features(todelete,data):
    """
    This function will drop columns from the data whose majority category covers more than 99% of occurences.
    """
    # find features where there are hardly any unique values
    # TODO learn more about this and possibly change variable names/appraoch
    sk_df = pd.DataFrame([{'column': c, 'uniq': data[c].nunique(), 'skewness': data[c].value_counts(normalize=True).values[0] * 100} for c in data.columns])
    sk_df = sk_df.sort_values('skewness', ascending = False)
    todelete.extend(sk_df[sk_df.skewness> 99].column.tolist())

    return todelete

<<<<<<< HEAD
def remove_correlated_features(todelete,data):
=======
def remove_correlated_features(todelete,train):
>>>>>>> e45101a122edb39597650881ad28bcf374dbbb52
	"""
	Removes features that have high correlation with other features.
	"""
	corrmx = data.corr(method='pearson')
	cols = list(corrmx.columns)
	j = corrmx.shape[1]
	newtest = corrmx.to_numpy()
	col1=[]
	col2=[]
	corr=[]
	for x in range(j):
		for y in range(x+1, j):
			if abs(newtest[x,y]) > .90:
				col1.append(cols[x])
				col2.append(cols[y])
				corr.append(newtest[x,y])
<<<<<<< HEAD
				target_corrs = data[[cols[x],cols[y],'HasDetections']].corr()
=======
				target_corrs = train[[cols[x],cols[y],'HasDetections']].corr()
>>>>>>> e45101a122edb39597650881ad28bcf374dbbb52
				if target_corrs.iloc[0,2] >= target_corrs.iloc[1,2]:
					todelete.append(list(target_corrs.columns)[0])
				else:
					todelete.append(list(target_corrs.columns)[1])
	result = pd.DataFrame(np.column_stack([col1,col2,corr]),
						columns=['Feature 1','Feature 2','Correlation'])
	return todelete
<<<<<<< HEAD


def impute_missing_data(data):
	#fills in numerical columns with mean
	numerics = ['int16', 'int32', 'int64', 'float16', 'float32','float64']
	numerical_cols = data.select_dtypes(include=numerics).columns.tolist()
	for col in numerical_cols:
		data[col] = data[col].fillna((data[col].mean()))
	
	# fills in categorical columns with mode (most common category)
	data.fillna(data.mode().iloc[0],inplace=True)

	#replaces any remaining empty data with NaN and deletes the row
	data.replace(['',np.inf,-np.inf],np.nan,inplace=True)
	data.dropna(how='any',inplace=True)

	# checks if any more NaN in dataset, should print nothing
	test = (~data.isin([np.nan,np.inf,-np.inf]).any(1))
=======


def impute_missing_data(train):
	#fills in numerical columns with mean
	numerics = ['int16', 'int32', 'int64', 'float16', 'float32','float64']
	numerical_cols = train.select_dtypes(include=numerics).columns.tolist()
	for col in numerical_cols:
		train[col] = train[col].fillna((train[col].mean()))
	
	#fills in categorical columns with mode (most common category)
	train.fillna(train.mode().iloc[0],inplace=True)

	#replaces any remaining empty data with NaN and deletes therow
	train.replace(['',np.inf,-np.inf],np.nan,inplace=True)
	train.dropna(how='any',inplace=True)

	#checks if any more NaN in dataset, should print nothing
	test = (~train.isin([np.nan,np.inf,-np.inf]).any(1))
>>>>>>> e45101a122edb39597650881ad28bcf374dbbb52
	for x in range(len(test)):
		if test[x] == False:
			print(x)

def main():
<<<<<<< HEAD
	data = pd.read_csv("train.csv", nrows=30000)
	print("Removing Columns")
	todelete = []
	todelete = remove_missing_features(todelete,data)
	todelete = remove_skewed_features(todelete, data)
	todelete = remove_correlated_features(todelete, data)
	todelete = list(set(todelete))
	todelete.append("MachineIdentifier")
	data = data.drop(todelete,axis=1)
	#Left with 63 rows.

	print("Imputing Data")
	impute_missing_data(data)

	print("One Hot Encoding")
	data = pd.get_dummies(data)

	x = data.iloc[:,:-1]
	y = data.iloc[:,-1:].values.ravel()
	xTrain,xTest,yTrain,yTest = train_test_split(x,y,random_state=334,
				stratify=y)

	"""Sample Run with Algorithms, testing if preprocessing is correct"""
	print("Training Random Forest")
	rf = RandomForestClassifier(n_estimators=100)
	rf.fit(xTrain,yTrain)
	yHat = rf.predict(xTest)
	print(roc_auc_score(yTest,yHat))
	print("Training Decision Tree")
	dt = DecisionTreeClassifier()
	dt.fit(xTrain,yTrain)
	yHat = dt.predict(xTest)
	print(roc_auc_score(yTest,yHat))
=======
	Train = pd.read_csv("train.csv", nrows=5000)
	todelete = []
	todelete = remove_missing_features(todelete,Train)
	todelete = remove_skewed_features(todelete, Train)
	todelete = remove_correlated_features(todelete, Train)
	todelete = list(set(todelete))

	Train = Train.drop(todelete,axis=1)
	#Left with 63 rows.
	impute_missing_data(Train)
	enc = OneHotEncoder()
	enc.fit(Train)
>>>>>>> e45101a122edb39597650881ad28bcf374dbbb52

if __name__ == "__main__":
	main()
