from sklearn.preprocessing import OneHotEncoder
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def remove_missing_features(todelete, train):
	"""
	Removes features with large amounts of missing data.
	"""
	missing = []
	for x in range(0, len(train.columns)):
		totals = (sum (train.iloc[:,x].isnull()) ) / len(train)
		missing.append(totals)
		if totals > .99:
			todelete.append(train.columns[x])
	return todelete

def remove_skewed_features(todelete,data):
    """
    This function will drop columns from the data whose majority category covers more than 99% of occurences.
    """
    # find features where there are hardly any unique values
    # TODO learn more about this and possibly change variable names/appraoch
    sk_df = pd.DataFrame([{'column': c, 'uniq': data[c].nunique(), 'skewness': data[c].value_counts(normalize=True).values[0] * 100} for c in data.columns])
    sk_df = sk_df.sort_values('skewness', ascending = False)
    todelete.extend(sk_df[sk_df.skewness> 99].column.tolist())

    return todelete

def remove_correlated_features(todelete,train):
	"""
	Removes features that have high correlation with other features.
	"""
	corrmx = train.corr(method='pearson')
	cols = list(corrmx.columns)
	j = corrmx.shape[1]
	newtest = corrmx.to_numpy()
	col1=[]
	col2=[]
	corr=[]
	for x in range(j):
		for y in range(x+1, j):
			if abs(newtest[x,y]) > .75:
				col1.append(cols[x])
				col2.append(cols[y])
				corr.append(newtest[x,y])
				target_corrs = train[[cols[x],cols[y],'HasDetections']].corr()
				if target_corrs.iloc[0,2] >= target_corrs.iloc[1,2]:
					todelete.append(list(target_corrs.columns)[0])
				else:
					todelete.append(list(target_corrs.columns)[1])
	result = pd.DataFrame(np.column_stack([col1,col2,corr]),
						columns=['Feature 1','Feature 2','Correlation'])
	return todelete


def impute_missing_data(train):
	#fills in numerical columns with mean
	numerics = ['int16', 'int32', 'int64', 'float16', 'float32','float64']
	numerical_cols = train.select_dtypes(include=numerics).columns.tolist()
	for col in numerical_cols:
		train[col] = train[col].fillna((train[col].mean()))
	
	#fills in categorical columns with mode (most common category)
	train.fillna(train.mode().iloc[0],inplace=True)

	#replaces any remaining empty data with NaN and deletes therow
	train.replace(['',np.inf,-np.inf],np.nan,inplace=True)
	train.dropna(how='any',inplace=True)

	#checks if any more NaN in dataset, should print nothing
	test = (~train.isin([np.nan,np.inf,-np.inf]).any(1))
	for x in range(len(test)):
		if test[x] == False:
			print(x)

def main():
	Train = pd.read_csv("train.csv", nrows=5000)
	todelete = []
	todelete = remove_missing_features(todelete,Train)
	todelete = remove_skewed_features(todelete, Train)
	todelete = remove_correlated_features(todelete, Train)
	todelete = list(set(todelete))

	Train = Train.drop(todelete,axis=1)
	#Left with 63 rows.
	impute_missing_data(Train)
	enc = OneHotEncoder()
	enc.fit(Train)

if __name__ == "__main__":
	main()
