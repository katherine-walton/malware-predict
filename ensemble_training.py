from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve,auc
import numpy as np
import pandas as pd
import preprocessing


def optimal_dt(xTrain,yTrain):
	"""
	Runs randomized search to find best hyperparameters.
	Takes 2 minutes to run.
	"""
	tuned_parameters = {
		'criterion': ['entropy','gini'],
		'max_depth': np.linspace(1,28,28,endpoint=True),
		'min_samples_leaf': list(range(1,41+1,2)),
		'max_features': [0.25]
	}
	dt = DecisionTreeClassifier()
	clf = RandomizedSearchCV(estimator=dt, cv=5, n_iter=50,
		param_distributions=tuned_parameters, verbose=2,
		random_state=334, n_jobs=-1)
	clf.fit(xTrain,yTrain)
	print(clf.best_params_)
	"""OUTPUT
	{'min_samples_leaf': 3, 'max_features': 0.25, 
	'max_depth': 18.0, 'criterion': 'gini'}
	"""


def optimal_rf(xTrain,yTrain):
	"""
	Runs randomized search to find best hyperparameters.
	Takes one hour to run.
	"""
	tuned_parameters = {
		'n_estimators': [20,30,40,50,60,70,90,100],
		'max_features': [0.25],
		'criterion': ['entropy','gini'],
		'max_depth': np.linspace(1, 32, 32, endpoint=True),
		'min_samples_leaf':list(range(1,41+1,2)),
		'oob_score': [True]
	}
	rf = RandomForestClassifier()
	clf = RandomizedSearchCV(estimator=rf,
			param_distributions=tuned_parameters, 
			cv=5,n_iter=50,verbose=2,random_state=334,n_jobs=-1)
	clf.fit(xTrain, yTrain)
	print(clf.best_params_)
	"""
	OUTPUT:
	{'oob_score': True, 'n_estimators': 100, 'min_samples_leaf': 7, 
	'max_features': 0.25, 'max_depth': 16.0, 'criterion': 'entropy'}
	"""


def main():
	xTrain,yTrain,xTest,yTest,xValidation,yValidation = preprocessing.main()
	#optimal_rf(xTrain,yTrain)
	#optimal_dt(xTrain,yTrain)

	print("Training Random Forest")
	rf = RandomForestClassifier(oob_score=True,n_estimators=100,
					min_samples_leaf=7, max_features=0.25,max_depth=16,
					criterion='entropy', random_state=334)
	rf.fit(xTrain,yTrain)
	rf_xTrain_validation = rf.predict(xTest)
	rf_xTest_validation = rf.predict(xValidation)

	print("Training Decision Tree")
	dt = DecisionTreeClassifier(min_samples_leaf=3, max_features=0.25,
					max_depth=18, criterion='gini',random_state=334)
	dt.fit(xTrain,yTrain)
	dt_xTrain_validation = dt.predict(xTest)
	dt_xTest_validation = dt.predict(xValidation)

	print("Training Naive Bayes")
	mnb = MultinomialNB()
	mnb.fit(xTrain,yTrain)
	mnb_xTrain_validation = mnb.predict(xTest)
	mnb_xTest_validation = mnb.predict(xValidation)

	# xgb = XGBClassifier(num_rounds=1,learning_rate=1,max_depth=3)
	# xgb.fit(xTrain,yTrain)
	# xgb_yHat = xgb.predict(xTest)
	# xgb_yVal = mnb.predict(xValidation)

	print("Training Ensemble")
	validationTrain = {
		'Random Forest': rf_xTrain_validation,
		'Decision Tree': dt_xTrain_validation,
		'Naive Bayes': mnb_xTrain_validation
	}

	validationTrain = pd.DataFrame(validationTrain)
	lr = LogisticRegression(random_state=334,solver='lbfgs')
	lr.fit(validationTrain,yTest)

	validationTest = {
		'Random Forest': rf_xTest_validation,
		'Decision Tree': dt_xTest_validation,
		'Naive Bayes': mnb_xTest_validation
	}
	validationTest = pd.DataFrame(validationTest)
	lr_yHat = lr.predict_proba(validationTest)[:,1]

	fpr,tpr,thresholds = roc_curve(yValidation,lr_yHat)
	roc_auc = auc(fpr,tpr)
	
	print("AUC: %f" % roc_auc)

if __name__ == "__main__":
	main()